
# gpt-2

Code from the paper ["Language Models are Unsupervised Multitask Learners"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).

See more details in OpenAI [blog post](https://blog.openai.com/better-language-models/).

## Information

This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.
I have added the Model downloading code(download_model.py) to download parameters from 117M to 1558M and test code(run.py) to test the model by having convation with the model you trained.

[MIT](./LICENSE)
